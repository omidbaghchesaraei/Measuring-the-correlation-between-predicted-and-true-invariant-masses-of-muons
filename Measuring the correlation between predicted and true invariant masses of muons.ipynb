{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rpy2==3.5.1"
      ],
      "metadata": {
        "id": "u-kgn1Fe_NFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1laCzAs91k2I"
      },
      "outputs": [],
      "source": [
        "# activate R magic\n",
        "%load_ext rpy2.ipython\n",
        "import rpy2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "library(tidyverse) "
      ],
      "metadata": {
        "id": "Sazsb97X_G27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "install.packages(\"neuralnet\") "
      ],
      "metadata": {
        "id": "1HLVwQGN-kMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "library(neuralnet)"
      ],
      "metadata": {
        "id": "HCiXduD5-t_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset contains 100k dimuon events selected from the Mu dataset from Run2010B. Each line corresponds to an event. The events in this derived dataset were selected because of the presence of precisely two muons with invariant mass between 2-110 GeV, one of which is a high-quality \"global\" muon. \n",
        "\n",
        "**Dataset semantics**:\n",
        "\n",
        "\n",
        "*   Run : The run number of the event.\n",
        "\n",
        "*   Event : The event number.\n",
        "\n",
        "*   E : The total energy of the muon (GeV).\n",
        "\n",
        "*   px,py,pz : The components of the momemtum of the muon (GeV).\n",
        "\n",
        "*   pt : The transverse momentum of the muon (GeV).\n",
        "*   eta : The pseudorapidity of the muon.\n",
        "\n",
        "\n",
        "*   phi : The phi angle of the muon (rad).\n",
        "\n",
        "\n",
        "*   Q : The charge of the muon.\n",
        "\n",
        "\n",
        "*   M : The invariant mass of two muons (GeV).\n",
        "\n",
        "\n",
        "Source: [opendata.cern](http://opendata.cern.ch/record/700)\n",
        "\n"
      ],
      "metadata": {
        "id": "zMcrot2TBZIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring and preparing the data"
      ],
      "metadata": {
        "id": "-043tYR1UpIf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UUMUEEhx4_Ol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd4e5ee-efe0-4762-ee38-a50b9c920331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'data.frame':\t100000 obs. of  21 variables:\n",
            " $ Run  : int  165617 165617 165617 165617 165617 165617 165617 165617 165617 165617 ...\n",
            " $ Event: int  74601703 75100943 75587682 75660978 75947690 74570517 74697773 74704205 75167029 75206813 ...\n",
            " $ type1: chr  \"G\" \"G\" \"G\" \"G\" ...\n",
            " $ E1   : num  9.7 6.2 19.29 7.04 7.28 ...\n",
            " $ px1  : num  -9.51 -4.267 -4.212 -6.327 0.103 ...\n",
            " $ py1  : num  0.366 0.457 -0.652 -0.269 -5.533 ...\n",
            " $ pz1  : num  1.86 -4.48 18.81 3.08 -4.72 ...\n",
            " $ pt1  : num  9.52 4.29 4.26 6.33 5.53 ...\n",
            " $ eta1 : num  0.195 -0.912 2.191 0.469 -0.774 ...\n",
            " $ phi1 : num  3.1 3.04 -2.99 -3.1 -1.55 ...\n",
            " $ Q1   : int  -1 -1 -1 -1 -1 -1 -1 1 -1 -1 ...\n",
            " $ type2: chr  \"G\" \"G\" \"G\" \"G\" ...\n",
            " $ E2   : num  9.76 9.67 9.82 5.59 7.32 ...\n",
            " $ px2  : num  7.328 7.274 4.344 4.475 -0.399 ...\n",
            " $ py2  : num  -1.152 -2.821 -0.473 0.849 6.941 ...\n",
            " $ pz2  : num  6.35 -5.71 8.8 -3.23 2.28 ...\n",
            " $ pt2  : num  7.42 7.8 4.37 4.55 6.95 ...\n",
            " $ eta2 : num  0.776 -0.679 1.45 -0.66 0.323 ...\n",
            " $ phi2 : num  -0.156 -0.37 -0.109 0.188 1.628 ...\n",
            " $ Q2   : int  1 1 1 1 1 1 1 -1 1 1 ...\n",
            " $ M    : num  17.49 11.55 9.16 12.48 14.32 ...\n"
          ]
        }
      ],
      "source": [
        "%%R\n",
        "df <- read.csv(\"/content/drive/MyDrive/Dimuon_DoubleMu.csv\", stringsAsFactors=FALSE) \n",
        "str(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 21 variables in the data frame correspond to the 20 features and one\n",
        "outcome we expected, although a problem has become apparent. Neural networks\n",
        "work best when the input data are scaled to a narrow range around zero, and here\n",
        "we see values ranging anywhere from zero up to over 75,000,000. Typically, the solution to this problem is to rescale the data with a normalizing or\n",
        "standardization function. I'd like to get a subset of the original dataset first."
      ],
      "metadata": {
        "id": "Tr2ZX6vKVEqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "df_subset <- subset(df, select=c('E1','E2','pt1', 'pt2', 'M'))\n",
        "str(df_subset)"
      ],
      "metadata": {
        "id": "xqQk_m-yKjkr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9335c2b3-0370-4958-c207-cb61bbd96adb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'data.frame':\t100000 obs. of  5 variables:\n",
            " $ E1 : num  9.7 6.2 19.29 7.04 7.28 ...\n",
            " $ E2 : num  9.76 9.67 9.82 5.59 7.32 ...\n",
            " $ pt1: num  9.52 4.29 4.26 6.33 5.53 ...\n",
            " $ pt2: num  7.42 7.8 4.37 4.55 6.95 ...\n",
            " $ M  : num  17.49 11.55 9.16 12.48 14.32 ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taking a closer look at these features"
      ],
      "metadata": {
        "id": "fTN4_C9tcIb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "summary(df_subset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBdJlpnpcca9",
        "outputId": "c48bd030-13b4-4c98-b5a6-eca332a25040"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       E1                 E2                pt1                pt2          \n",
            " Min.   :   2.711   Min.   :   2.634   Min.   :  0.2663   Min.   :  0.6828  \n",
            " 1st Qu.:   8.777   1st Qu.:   6.669   1st Qu.:  5.0190   1st Qu.:  4.9325  \n",
            " Median :  13.017   Median :   9.505   Median :  6.9436   Median :  6.8640  \n",
            " Mean   :  20.414   Mean   :  14.782   Mean   : 10.0716   Mean   : 10.0077  \n",
            " 3rd Qu.:  21.071   3rd Qu.:  15.749   3rd Qu.: 10.4396   3rd Qu.: 10.3748  \n",
            " Max.   :8684.880   Max.   :1604.970   Max.   :366.4990   Max.   :528.4340  \n",
            "       M           \n",
            " Min.   :  0.3002  \n",
            " 1st Qu.:  4.4922  \n",
            " Median : 12.4110  \n",
            " Mean   : 17.6911  \n",
            " 3rd Qu.: 19.4227  \n",
            " Max.   :299.2020  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation â€“ normalizing numeric data"
      ],
      "metadata": {
        "id": "WRcBGuUsdJz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To normalize these features, we need to create a normalize() function in R. This\n",
        "function takes a vector x of numeric values, and for each value in x, subtract the minimum value in x and divide by the range of values in x. Finally, the resulting vector is returned. The code for the function is as follows:"
      ],
      "metadata": {
        "id": "W6rJ-QNIdY2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "normalize <- function(x) {\n",
        " return((x - min(x)) / (max(x) - min(x)))\n",
        " }"
      ],
      "metadata": {
        "id": "QICqSg_4FRUs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now apply the normalize() function to the numeric features in our data\n",
        "frame. Rather than normalizing each of the 5 numeric variables individually, we\n",
        "will use one of R's functions to automate the process. I can use lapply() to apply normalize() to each feature in the subset. "
      ],
      "metadata": {
        "id": "gU9eUvKDeEuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "df_subset_norm <- as.data.frame(lapply(df_subset, normalize))"
      ],
      "metadata": {
        "id": "asFg8K3PFRHm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To confirm that the normalization worked, we can see that the minimum and\n",
        "maximum M are now 0 and 1, respectively:"
      ],
      "metadata": {
        "id": "7Av7Ih7rfbF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "summary(df_subset_norm$M)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnbRs7N1FRCX",
        "outputId": "0eaa97c9-f153-4925-8302-48be29143eaa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
            "0.00000 0.01402 0.04052 0.05818 0.06398 1.00000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting dataset into train and test\n"
      ],
      "metadata": {
        "id": "WH5TtJZZhQuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "df_subset_train <- df_subset_norm[1:80000, ]\n",
        "df_subset_test <- df_subset_norm[80001:100000, ] "
      ],
      "metadata": {
        "id": "0NMj5udYFQ_Y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the training dataset to build the neural network and the testing dataset to evaluate how well the model generalizes to future results. As it is easy to overfit a neural network, this step is very important."
      ],
      "metadata": {
        "id": "R3uJ-nbVhpat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a model on the data\n",
        "I'll begin by training the simplest multilayer feedforward network with only\n",
        "a single hidden node:"
      ],
      "metadata": {
        "id": "h9R5b_PZi6ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "df_subset_model <- neuralnet(M ~ E1 + E2 + pt1 + pt2, data = df_subset_train, hidden = 1)"
      ],
      "metadata": {
        "id": "9i4Uj_eAGsvl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating model performance\n",
        "To estimate our model's performance, we can use the `compute()` function to\n",
        "generate predictions on the testing dataset:"
      ],
      "metadata": {
        "id": "0sDxGkJqkauS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "model_results <- compute(df_subset_model, df_subset_test[1:4]) "
      ],
      "metadata": {
        "id": "x2ORA_CHGsIP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `compute()` function returns a list with two components: `neurons`, which  stores the neurons for each layer in the network, and `net.results`, which stores the predicted values. I'll want the latter:"
      ],
      "metadata": {
        "id": "qGX1HD4WmA1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "predicted_M <- model_results$net.result"
      ],
      "metadata": {
        "id": "RJ4pgKZ-GrZ8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because this is a numeric prediction problem rather than a classification problem, I cannot use a confusion matrix to examine model accuracy. Instead, I must measure the correlation between my predicted M and the true value. This provides an insight into the strength of the linear association between the two variables. The `cor()` function is used to obtain a correlation between two\n",
        "numeric vectors:"
      ],
      "metadata": {
        "id": "RNGLfciyyWXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R \n",
        "cor(predicted_M, df_subset_test$M) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AT6u7D-MVWy",
        "outputId": "688293f0-c1bc-4ec0-8dab-7bd38e2384f7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         [,1]\n",
            "[1,] 0.856717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlations close to 1 indicate strong linear relationships between two variables. Therefore, the correlation here of about `0.86` indicates a fairly strong relationship. This implies that my model is doing a fairly good job, even with only a single hidden node."
      ],
      "metadata": {
        "id": "05_QwA8czzt2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}